\section{Background}
\label{S:lit}
First, a general overview of the research on pedestrian crossing behaviour is presented. As per the scope of the study, we mainly focus on studies on crossing behaviours at unsignalized and mid-block crossings. Survival models have primarily been used in such studies. Thus, a brief review on the background of survival analysis methods and their transformations using deep learning is provided. Then we discuss the literature on interpretability of machine learning models, especially those applicable to transportation research. As neural networks are generally considered a \textit{black box}, developing a systematic interpretability method is essential in our study to recognize contributing factors to pedestrian wait time and infer insights for practical implications of the results. 
\subsection{Pedestrian unsignalized crossing behaviour}
Studies on gap time acceptance form a large part of pedestrian crossing behaviour research studies at unsignalized intersections and mid-block crossings. Das~\textit{et al.}~\cite{das2005walk} found that people waiting on medians accept shorter gaps more easily, compared to those waiting on the curb sides. Distribution of gaps was estimated in this study using parametric and non-parametric approaches. Oxley~\textit{et al.}~\cite{oxley2005crossing} focused on the effect of age on the ability to choose safe gap time. First, ANOVA was used to compare wait times of participants. According to their results, the primary contributing factor in deciding to cross, which can be interpreted as waiting time, appeared to be the distance to upcoming vehicle and time of arrival. Moreover, elderly participants appeared to select more unsafe gap times, given that their walking speeds were slower. In a relatively more recent work, Kadali~\textit{et al.}~\cite{kadali2014evaluation} used artificial neural networks to estimate pedestrians' gap acceptance at mid-block unmarked crossings and compared its performance to multiple regression models. Their results showed that ANN has a better prediction performance, being able to consider the effects of more number of variables. However, the authors mentioned the strength of regression models in such cases due to their ability to reflect the significance of contributing variable.

Systematic modelling of pedestrian wait time using cox proportional hazards (CPH) models dates back to early 2000's, where Hamed developed a CPH model to analyze the time pedestrians spend on the sidewalk before initiating a cross on mid-block crosswalks. Data used in this study were collected by observing and manual recordings, and interviews at pedestrian crossings. Among the factors analyzed, previous accident experiences, group size of people crossing, car ownership, trip purpose, gap time between cars and finally, age and gender were appeared to be the contributing factors to wait time of pedestrians in their sample size~\cite{hamed2001analysis}. Interestingly, parameters related to traffic were not found to be statistically significant in Hamed's study. In another application of survival models in pedestrian wait time analysis, Wang~\textit{et al.}~\cite{wang2011individual} investigated the effects of personal characteristics on crossing behaviour using a parametric survival analysis. The effect of age, trip purpose, safety awareness and conformity behaviour appeared to be the contributing factors to pedestrian wait time in this study. 

\subsection{Interaction of pedestrians and automated vehicles}
Despite the long history of research activities on pedestrian crossing behaviour, studies on their behaviour in the presence of automated vehicles are yet relatively new. Analyzing communication techniques between the two involved agents form a large part of these studies~\citep{millard2018pedestrians,clamann2017evaluation,rasouli2017agreeing,mahadevan2018communicating}. Although the effect of communication systems among pedestrians and automated vehicles have appeared to be positive according to most studies, parameters that were shown to be important in traditional studies on human-driven vehicles are yet contributing factors to the behaviour of pedestrians in the context of automated vehicles~\citep{pillai2017virtual,clamann2017evaluation}.

Perceived risk of automated vehicles by the pedestrians has been another direction in the literature of pedestrians and automated vehicles. Generally, it is expected that different people would react and behave differently to the concept of automated vehicles. By conducting several questionnaire surveys, Deb~\textit{et al.}~\cite{deb2017pedestrians} found younger people, males, people from urban areas, and people with higher personal innovativeness to have higher acceptance towards AVs. Similar results on the effect of age, gender, and risk-taking personality were achieved in questionnaires surveyed by Hulse~\textit{et al.}~\cite{hulse2018perceptions}. Jayaraman~\textit{et al.}~\cite{jayaraman2019pedestrian} investigated trust in automated vehicles by conducting a virtual reality study. By studying the role of traffic signals and driving behaviour on the trust of AV by pedestrians, they found that trust in AVs is strongly correlated with pedestrian gaze, pedestrian distance to collision, and pedestrian jaywalking time. Reading through the relatively new literature on pedestrian behaviour in the presence of AVs, a major research gap seems to exist, which we try to address in this study: Lack of research on the performance of variables that were significant in traditional crossing behaviour studies, within the new automated context. Variables such as street width, road design, and traffic variables have not been a major part of the research on pedestrian-AV interaction.  Failure to account for these variables before the transition towards automated environments can lead to inefficient or dangerous designs, where vehicles regard pedestrians more conservative or reckless than they really are~\citep{rasouli2019autonomous}.

\subsection{Survival models and data-driven machine learning}
In survival analysis, the aim is to analyze the time until an event occurs. A collection of statistical modelling methods can be designed for this purpose. The most common methods for survival analysis are the Kaplan-Meier model~\citep{kaplan1958nonparametric} and Cox Proportional Hazards (CPH) model~\citep{cox1972regression}.
The Kaplan-Meier model is a non-parametric model for estimating the survival function in homogeneous groups. This model is very easy to implement, but is unable to account for individuals. To take into account covariate vectors and compute survival functions for individuals, CPH model is a conventional solution. CPH model is a semi-parametric model that assumes the time component and the covariate component of hazard function to be proportional. Hazard function is defined as the risk of an event with explanatory variables (covariates) $Z$ to occur, at time $t$. Despite being the common method of survival analysis for years, the assumptions made in Cox Proportional Hazards are not always true and have limitations. It assumes a) a linear combination of covariates within the hazard function and b) a constant relative effect of covariates over time.
To address these issues, Yu~\textit{et al.}~\cite{yu2011learning} introduced a Multi-Task Logistic Regression model. Their model can be interpreted as having different time intervals with different logistic regression models that estimate the probability of occurrence of an event in a interval. Although Multi-Task Logistic Regression models address some of the problems of the previous methods, they still remain linear, thus they cannot capture the nonlinear complexities within the data. Several researchers, particularly in the field of medical sciences, have tried to address this issue by implementing a deep learning approach \citep{faraggi1995neural,katzman2016deep,luck2017deep}.
Faraggi~\textit{et al.}~\cite{faraggi1995neural} first incorporated a feed-forward neural network as a shift to nonlinear proportional hazards model. In their model, they replaced the linear combination of covariates with the output of a neural network with one output node. However, research followed by suggested that their network did not perform better than linear Cox Proportional Hazards models~\citep{mariani1997prognostic}. As the mentioned work were done prior to the outburst of modern deep learning algorithms, Katzman~\textit{et al.}~\cite{katzman2016deep} decided to test the performance of more recent deep networks on survival models. They added fully connected and dropout layers and outperformed the performance of previous linear cox proportional hazards models. As a suggested future direction, the authors suggested implementing architectures like Convolutional Neural Networks to enable estimating waiting time directly from medical images. 

Strong performances of survival analysis models in different areas, along with their relatively widespread usage in pedestrian wait time modelling, made them a suitable base model for our study. In an earlier study in our lab, we started analyzing crossing behaviour by studying distracted pedestrian's waiting time using Cox Proportional Hazards models, and we addressed various contributing distraction factors~\citep{kalatian2018cox}. Later, in~\citep{kalatian2019deepwait}, we introduced \textit{Deepwait}, which is a neural network based extension of CPH, empowered with a feature selection algorithm. However, despite the improvements in accuracy we obtained by using Deepwait, lack of a interpretability mechanism makes the application of such models limited, especially as learning the contributing factors are of vital importance. To better understand the problem of interpretability in machine learning models, and to be able to propose policy recommendations based on the results of our model, a review on the existing literature of machine learning interpretability is presented in the following subsection.
\subsection{Interpretation in machine learning}
Widespread application of machine learning methods for decision making and policy planning requires a high level of interpretability for often complex, dense, and deep networks. Despite high prediction accuracies obtained by machine learning models in recent years, interpretability and explainability of the algorithms are yet considered as a cumbersome and in some cases, even unnecessary task. Major arguments against interpretability of machine learning models are the notion that performance of a model is more important than model interpretability and, the burden of understanding the model can prevent adoption of machine learning models~\citep{lipton2016mythos}. Nevertheless, the trade off between model complexity and explainability may weigh in favor of using traditional explainable models in some areas where gaining information on \textit{why?} and \textit{how?} the model works is as important as the predictivity of the model.

To fill the existing void in machine learning models, various approaches to make machine learning models explainable have emerged in recent years. The \textit{black-box} nature of machine learning algorithms can be addressed by model-agnostic methods, which are post-hoc interpretability methods that rely solely on input and output of the models, disregarding their structure ~\citep{molnar2019interpretable}. Mainly practiced model-agnostic methods include sensitivity analysis plots~\citep{goldstein2015peeking}, gradient boosting feature importance methods~\citep{friedman2001greedy}, feature permutation methods~\citep{fisher2018all}, surrogate local interpretable estimators~\citep{ribeiro2016should}, and game theoretic based approaches~\citep{vstrumbelj2014explaining,lundberg2017unified}. In transportation, not so many studies have addressed the interpretability of the models despite the relatively large number of works on machine learning and deep learning applications in the field. In the interpretability of choice analysis for instance, Hagenauer~\textit{et al.}~\cite{hagenauer2017comparative} used a permutation-based variable importance method upon their DNN model to analyze the performance of the model when the values of a variable changes randomly. In \citep{wang2018deep}, by considering the inputs to the Softmax layer as the utility functions, authors compute economic information using DNNs. They conclude that economical information aggregated over the population can be reliably derived from DNN. However, disaggregate information for individuals still lacks the reliability acquired by discrete choice models. Wong~\textit{et al.}~\cite{wong2020bi} used Jacobian determinant of generative models to calculate the elasticity of mode choice with respect to different explanatory variables. Jacobian matrix, in this study, was generated for each instance of all the conditional outputs, and density of elasticises was estimated across the data points. Use of Jacobian matrix, however, is computationally expensive and not guaranteed to observe the interactive effects of variables. For prediction and estimation tasks, Das~\textit{et al.}~\cite{das2019interpretable} used partial dependency plots to intuitively show the effects of different variables and detect contributing factors on predicting traffic volumes. Most of the existing work in transportation rely on intuitive and illustration-based interpretability frameworks without considering the interactive effects of variables in a systematic manner. To address this issue, more recent and advanced methods of game-theoretic based interpretability methods will be used in this study to find the contributing factors to pedestrians' waiting time before crossing.

